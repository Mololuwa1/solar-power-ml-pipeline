{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Power Generation Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook implements the data preprocessing pipeline for solar power generation prediction.\n",
    "Target variable: generation(kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = '/home/ubuntu/upload/'\n",
    "OUTPUT_DIR = '/home/ubuntu/processed_data/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_power_generation_data():\n",
    "    \"\"\"Load and combine all power generation data\"\"\"\n",
    "    all_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n",
    "    \n",
    "    # Filter power generation files\n",
    "    power_files = [f for f in all_files if not any(weather in f for weather in \n",
    "                   ['Temperature', 'Humidity', 'Irradiance', 'Wind', 'Visibility', \n",
    "                    'SeaLevelPressure', 'RelativeHumidity', 'Rainfall']) \n",
    "                   and not 'Inverter' in f]\n",
    "    \n",
    "    power_data_list = []\n",
    "    \n",
    "    for file in power_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            df['Time'] = pd.to_datetime(df['Time'])\n",
    "            df['station'] = os.path.basename(file).replace('.csv', '')\n",
    "            power_data_list.append(df)\n",
    "            print(f\"Loaded {os.path.basename(file)}: {df.shape[0]} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    # Combine all power data\n",
    "    combined_power = pd.concat(power_data_list, ignore_index=True)\n",
    "    combined_power = combined_power.sort_values(['station', 'Time']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nTotal power generation data: {combined_power.shape[0]:,} records\")\n",
    "    print(f\"Stations: {combined_power['station'].nunique()}\")\n",
    "    print(f\"Date range: {combined_power['Time'].min()} to {combined_power['Time'].max()}\")\n",
    "    \n",
    "    return combined_power\n",
    "\n",
    "def load_weather_data():\n",
    "    \"\"\"Load and combine all weather data\"\"\"\n",
    "    all_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))\n",
    "    all_files.extend(glob.glob(os.path.join(DATA_DIR, '*.xlsx')))\n",
    "    \n",
    "    # Filter weather files\n",
    "    weather_files = [f for f in all_files if any(weather in f for weather in \n",
    "                    ['Temperature', 'Humidity', 'Irradiance', 'Wind', 'Visibility', \n",
    "                     'SeaLevelPressure', 'RelativeHumidity', 'Rainfall'])]\n",
    "    \n",
    "    weather_data = {}\n",
    "    \n",
    "    for file in weather_files:\n",
    "        weather_type = os.path.basename(file).split('_')[0]\n",
    "        year = os.path.basename(file).split('_')[1].replace('.csv', '').replace('.xlsx', '')\n",
    "        \n",
    "        try:\n",
    "            if file.endswith('.xlsx'):\n",
    "                df = pd.read_excel(file)\n",
    "            else:\n",
    "                df = pd.read_csv(file)\n",
    "            \n",
    "            df['Time'] = pd.to_datetime(df['Time'])\n",
    "            \n",
    "            if weather_type not in weather_data:\n",
    "                weather_data[weather_type] = []\n",
    "            \n",
    "            weather_data[weather_type].append(df)\n",
    "            print(f\"Loaded {weather_type}_{year}: {df.shape[0]} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    # Combine weather data by type\n",
    "    combined_weather = {}\n",
    "    for weather_type, dfs in weather_data.items():\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values('Time').reset_index(drop=True)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        combined_df = combined_df.drop_duplicates(subset=['Time']).reset_index(drop=True)\n",
    "        \n",
    "        combined_weather[weather_type] = combined_df\n",
    "        print(f\"Combined {weather_type}: {combined_df.shape[0]:,} records\")\n",
    "    \n",
    "    return combined_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load power generation data\n",
    "print(\"Loading power generation data...\")\n",
    "power_data = load_power_generation_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Loading weather data...\")\n",
    "weather_data = load_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess power data quality\n",
    "print(\"=== POWER DATA QUALITY ===\")\n",
    "print(f\"Shape: {power_data.shape}\")\n",
    "print(f\"Missing values:\")\n",
    "print(power_data.isnull().sum())\n",
    "print(f\"\\nGeneration statistics:\")\n",
    "print(power_data['generation(kWh)'].describe())\n",
    "\n",
    "# Check for negative values\n",
    "negative_gen = (power_data['generation(kWh)'] < 0).sum()\n",
    "print(f\"\\nNegative generation values: {negative_gen}\")\n",
    "\n",
    "# Check for outliers (values > 99th percentile)\n",
    "gen_99th = power_data['generation(kWh)'].quantile(0.99)\n",
    "outliers = (power_data['generation(kWh)'] > gen_99th).sum()\n",
    "print(f\"Values above 99th percentile ({gen_99th:.2f}): {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess weather data quality\n",
    "print(\"=== WEATHER DATA QUALITY ===\")\n",
    "for weather_type, df in weather_data.items():\n",
    "    print(f\"\\n{weather_type}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  Date range: {df['Time'].min()} to {df['Time'].max()}\")\n",
    "    \n",
    "    # Get numeric column\n",
    "    numeric_col = [col for col in df.columns if col != 'Time'][0]\n",
    "    print(f\"  {numeric_col} range: {df[numeric_col].min():.2f} to {df[numeric_col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weather Data Resampling and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_weather_data(weather_data, target_freq='15T'):\n",
    "    \"\"\"Resample weather data to match power generation frequency\"\"\"\n",
    "    resampled_weather = {}\n",
    "    \n",
    "    for weather_type, df in weather_data.items():\n",
    "        # Get the numeric column name\n",
    "        numeric_col = [col for col in df.columns if col != 'Time'][0]\n",
    "        \n",
    "        # Set Time as index for resampling\n",
    "        df_indexed = df.set_index('Time')\n",
    "        \n",
    "        # Resample to target frequency (15 minutes) using mean\n",
    "        resampled = df_indexed.resample(target_freq).agg({\n",
    "            numeric_col: ['mean', 'min', 'max', 'std']\n",
    "        })\n",
    "        \n",
    "        # Flatten column names\n",
    "        resampled.columns = [f\"{weather_type}_{stat}\" for stat in ['mean', 'min', 'max', 'std']]\n",
    "        \n",
    "        # Reset index to get Time as column\n",
    "        resampled = resampled.reset_index()\n",
    "        \n",
    "        resampled_weather[weather_type] = resampled\n",
    "        print(f\"Resampled {weather_type}: {resampled.shape[0]:,} records\")\n",
    "    \n",
    "    return resampled_weather\n",
    "\n",
    "# Resample weather data\n",
    "print(\"Resampling weather data to 15-minute intervals...\")\n",
    "resampled_weather = resample_weather_data(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df, time_col='Time'):\n",
    "    \"\"\"Create time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df['year'] = df[time_col].dt.year\n",
    "    df['month'] = df[time_col].dt.month\n",
    "    df['day'] = df[time_col].dt.day\n",
    "    df['hour'] = df[time_col].dt.hour\n",
    "    df['minute'] = df[time_col].dt.minute\n",
    "    df['day_of_week'] = df[time_col].dt.dayofweek\n",
    "    df['day_of_year'] = df[time_col].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_col].dt.isocalendar().week\n",
    "    \n",
    "    # Cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Solar position approximation\n",
    "    df['solar_elevation'] = np.sin(2 * np.pi * (df['day_of_year'] - 81) / 365) * 23.45\n",
    "    \n",
    "    # Time of day categories\n",
    "    df['is_daytime'] = ((df['hour'] >= 6) & (df['hour'] <= 18)).astype(int)\n",
    "    df['is_peak_sun'] = ((df['hour'] >= 10) & (df['hour'] <= 14)).astype(int)\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col, lags=[1, 2, 3, 4, 24, 48, 96]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col, windows=[4, 12, 24, 48, 96]):\n",
    "    \"\"\"Create rolling window features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for window in windows:\n",
    "        df[f'{target_col}_rolling_mean_{window}'] = df[target_col].rolling(window=window, min_periods=1).mean()\n",
    "        df[f'{target_col}_rolling_std_{window}'] = df[target_col].rolling(window=window, min_periods=1).std()\n",
    "        df[f'{target_col}_rolling_max_{window}'] = df[target_col].rolling(window=window, min_periods=1).max()\n",
    "        df[f'{target_col}_rolling_min_{window}'] = df[target_col].rolling(window=window, min_periods=1).min()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply time features to power data\n",
    "print(\"Creating time-based features...\")\n",
    "power_data_featured = create_time_features(power_data)\n",
    "print(f\"Added time features. New shape: {power_data_featured.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merge Power and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weather_data(power_df, weather_dict):\n",
    "    \"\"\"Merge weather data with power generation data\"\"\"\n",
    "    merged_df = power_df.copy()\n",
    "    \n",
    "    for weather_type, weather_df in weather_dict.items():\n",
    "        print(f\"Merging {weather_type} data...\")\n",
    "        \n",
    "        # Merge on Time\n",
    "        merged_df = pd.merge(merged_df, weather_df, on='Time', how='left')\n",
    "        \n",
    "        print(f\"  Merged shape: {merged_df.shape}\")\n",
    "        \n",
    "        # Check missing values after merge\n",
    "        weather_cols = [col for col in weather_df.columns if col != 'Time']\n",
    "        missing_after_merge = merged_df[weather_cols].isnull().sum().sum()\n",
    "        print(f\"  Missing values after merge: {missing_after_merge}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Merge weather data\n",
    "print(\"Merging weather data with power generation data...\")\n",
    "merged_data = merge_weather_data(power_data_featured, resampled_weather)\n",
    "print(f\"\\nFinal merged data shape: {merged_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in merged data\n",
    "print(\"=== MISSING VALUES IN MERGED DATA ===\")\n",
    "missing_summary = merged_data.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "print(missing_summary)\n",
    "\n",
    "# Calculate missing percentage\n",
    "missing_pct = (missing_summary / len(merged_data)) * 100\n",
    "print(\"\\nMissing percentages:\")\n",
    "print(missing_pct)\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(df, strategy='forward_fill'):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove target variable from imputation\n",
    "    if 'generation(kWh)' in numeric_cols:\n",
    "        numeric_cols.remove('generation(kWh)')\n",
    "    \n",
    "    print(f\"Handling missing values for {len(numeric_cols)} numeric columns...\")\n",
    "    \n",
    "    if strategy == 'forward_fill':\n",
    "        # Forward fill then backward fill\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    elif strategy == 'interpolate':\n",
    "        # Linear interpolation\n",
    "        df[numeric_cols] = df[numeric_cols].interpolate(method='linear')\n",
    "        # Fill any remaining NaNs\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    elif strategy == 'knn':\n",
    "        # KNN imputation\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply missing value handling\n",
    "print(\"\\nApplying missing value handling...\")\n",
    "cleaned_data = handle_missing_values(merged_data, strategy='interpolate')\n",
    "\n",
    "# Check if missing values are handled\n",
    "remaining_missing = cleaned_data.isnull().sum().sum()\n",
    "print(f\"Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Lag and Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by station and time for proper lag feature creation\n",
    "cleaned_data = cleaned_data.sort_values(['station', 'Time']).reset_index(drop=True)\n",
    "\n",
    "# Create lag and rolling features for each station separately\n",
    "print(\"Creating lag and rolling features for each station...\")\n",
    "featured_data_list = []\n",
    "\n",
    "for station in cleaned_data['station'].unique():\n",
    "    station_data = cleaned_data[cleaned_data['station'] == station].copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    station_data = create_lag_features(station_data, 'generation(kWh)', lags=[1, 2, 4, 24, 48])\n",
    "    \n",
    "    # Create rolling features\n",
    "    station_data = create_rolling_features(station_data, 'generation(kWh)', windows=[4, 12, 24, 48])\n",
    "    \n",
    "    # Add weather lag features (for key weather variables)\n",
    "    if 'Irradiance_mean' in station_data.columns:\n",
    "        station_data = create_lag_features(station_data, 'Irradiance_mean', lags=[1, 2, 4])\n",
    "    \n",
    "    if 'Temperature_mean' in station_data.columns:\n",
    "        station_data = create_lag_features(station_data, 'Temperature_mean', lags=[1, 2, 4])\n",
    "    \n",
    "    featured_data_list.append(station_data)\n",
    "    print(f\"Processed {station}: {station_data.shape}\")\n",
    "\n",
    "# Combine all station data\n",
    "final_featured_data = pd.concat(featured_data_list, ignore_index=True)\n",
    "print(f\"\\nFinal featured data shape: {final_featured_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Selection and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN in target variable\n",
    "final_featured_data = final_featured_data.dropna(subset=['generation(kWh)']).reset_index(drop=True)\n",
    "\n",
    "# Identify feature columns (exclude target, time, and station)\n",
    "exclude_cols = ['Time', 'generation(kWh)', 'power(W)', 'station']\n",
    "feature_cols = [col for col in final_featured_data.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Sample features: {feature_cols[:10]}\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_cols = []\n",
    "for col in feature_cols:\n",
    "    if np.isinf(final_featured_data[col]).any():\n",
    "        inf_cols.append(col)\n",
    "\n",
    "if inf_cols:\n",
    "    print(f\"\\nColumns with infinite values: {inf_cols}\")\n",
    "    # Replace infinite values with NaN and then fill\n",
    "    final_featured_data[inf_cols] = final_featured_data[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    final_featured_data[inf_cols] = final_featured_data[inf_cols].fillna(method='ffill').fillna(0)\n",
    "\n",
    "# Remove any remaining NaN values in features\n",
    "final_featured_data[feature_cols] = final_featured_data[feature_cols].fillna(0)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {final_featured_data.shape}\")\n",
    "print(f\"Missing values in features: {final_featured_data[feature_cols].isnull().sum().sum()}\")\n",
    "print(f\"Missing values in target: {final_featured_data['generation(kWh)'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for scaling\n",
    "X = final_featured_data[feature_cols].copy()\n",
    "y = final_featured_data['generation(kWh)'].copy()\n",
    "\n",
    "# Use RobustScaler to handle outliers\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)\n",
    "\n",
    "print(f\"Scaled features shape: {X_scaled.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset with metadata\n",
    "processed_data = final_featured_data.copy()\n",
    "processed_data[feature_cols] = X_scaled\n",
    "\n",
    "# Save processed data\n",
    "processed_data.to_csv(os.path.join(OUTPUT_DIR, 'processed_solar_data.csv'), index=False)\n",
    "print(f\"Saved processed data to {OUTPUT_DIR}processed_solar_data.csv\")\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'feature_columns': feature_cols,\n",
    "    'target_column': 'generation(kWh)',\n",
    "    'metadata_columns': ['Time', 'station'],\n",
    "    'total_features': len(feature_cols),\n",
    "    'total_samples': len(processed_data),\n",
    "    'stations': list(processed_data['station'].unique()),\n",
    "    'date_range': [str(processed_data['Time'].min()), str(processed_data['Time'].max())]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(OUTPUT_DIR, 'feature_info.json'), 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'feature_scaler.pkl'))\n",
    "print(f\"Saved feature scaler to {OUTPUT_DIR}feature_scaler.pkl\")\n",
    "\n",
    "print(f\"\\n=== PREPROCESSING COMPLETE ===\")\n",
    "print(f\"Final dataset: {processed_data.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Stations: {len(processed_data['station'].unique())}\")\n",
    "print(f\"Date range: {processed_data['Time'].min()} to {processed_data['Time'].max()}\")\n",
    "print(f\"Target range: {y.min():.4f} to {y.max():.4f} kWh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

