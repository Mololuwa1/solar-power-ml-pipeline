{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Power Generation Model Development and Training\n",
    "\n",
    "This notebook implements multiple regression models for solar power generation prediction:\n",
    "- XGBoost Regressor\n",
    "- Random Forest Regressor\n",
    "- Neural Network (MLP Regressor)\n",
    "\n",
    "Target variable: generation(kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = '/home/ubuntu/processed_data/'\n",
    "MODEL_DIR = '/home/ubuntu/models/'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "print(\"Loading processed data...\")\n",
    "data = pd.read_csv(os.path.join(DATA_DIR, 'processed_solar_data.csv'))\n",
    "data['Time'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# Load feature info\n",
    "with open(os.path.join(DATA_DIR, 'feature_info.json'), 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "\n",
    "feature_cols = feature_info['feature_columns']\n",
    "target_col = feature_info['target_column']\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"Date range: {data['Time'].min()} to {data['Time'].max()}\")\n",
    "print(f\"Stations: {data['station'].nunique()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(data[target_col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = data[feature_cols].copy()\n",
    "y = data[target_col].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"\\nMissing values in features: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in target: {y.isnull().sum()}\")\n",
    "\n",
    "# Remove any rows with missing values\n",
    "mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "X = X[mask].reset_index(drop=True)\n",
    "y = y[mask].reset_index(drop=True)\n",
    "data_clean = data[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter removing missing values:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split (Time-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split to avoid data leakage\n",
    "# Use 80% for training, 20% for testing\n",
    "split_date = data_clean['Time'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_mask = data_clean['Time'] <= split_date\n",
    "test_mask = data_clean['Time'] > split_date\n",
    "\n",
    "X_train = X[train_mask].reset_index(drop=True)\n",
    "X_test = X[test_mask].reset_index(drop=True)\n",
    "y_train = y[train_mask].reset_index(drop=True)\n",
    "y_test = y[test_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train ratio: {len(X_train) / len(X):.2%}\")\n",
    "print(f\"Test ratio: {len(X_test) / len(X):.2%}\")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter validation split:\")\n",
    "print(f\"Training set: {X_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost Regressor...\")\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_split)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "xgb_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train_split, y_train_pred_xgb)),\n",
    "    'train_mae': mean_absolute_error(y_train_split, y_train_pred_xgb),\n",
    "    'train_r2': r2_score(y_train_split, y_train_pred_xgb),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred_xgb)),\n",
    "    'val_mae': mean_absolute_error(y_val, y_val_pred_xgb),\n",
    "    'val_r2': r2_score(y_val, y_val_pred_xgb),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred_xgb)),\n",
    "    'test_mae': mean_absolute_error(y_test, y_test_pred_xgb),\n",
    "    'test_r2': r2_score(y_test, y_test_pred_xgb)\n",
    "}\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_model, os.path.join(MODEL_DIR, 'xgboost_model.pkl'))\n",
    "print(\"\\nXGBoost model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest Regressor...\")\n",
    "\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train_split)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "rf_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train_split, y_train_pred_rf)),\n",
    "    'train_mae': mean_absolute_error(y_train_split, y_train_pred_rf),\n",
    "    'train_r2': r2_score(y_train_split, y_train_pred_rf),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred_rf)),\n",
    "    'val_mae': mean_absolute_error(y_val, y_val_pred_rf),\n",
    "    'val_r2': r2_score(y_val, y_val_pred_rf),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred_rf)),\n",
    "    'test_mae': mean_absolute_error(y_test, y_test_pred_rf),\n",
    "    'test_r2': r2_score(y_test, y_test_pred_rf)\n",
    "}\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, os.path.join(MODEL_DIR, 'random_forest_model.pkl'))\n",
    "print(\"\\nRandom Forest model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural Network (MLP Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network (MLP Regressor)...\")\n",
    "\n",
    "# Scale features for neural network\n",
    "nn_scaler = StandardScaler()\n",
    "X_train_scaled = nn_scaler.fit_transform(X_train_split)\n",
    "X_val_scaled = nn_scaler.transform(X_val)\n",
    "X_test_scaled = nn_scaler.transform(X_test)\n",
    "\n",
    "# Neural Network parameters\n",
    "nn_params = {\n",
    "    'hidden_layer_sizes': (100, 50, 25),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.001,\n",
    "    'learning_rate': 'adaptive',\n",
    "    'max_iter': 500,\n",
    "    'random_state': 42,\n",
    "    'early_stopping': True,\n",
    "    'validation_fraction': 0.1\n",
    "}\n",
    "\n",
    "# Train Neural Network\n",
    "nn_model = MLPRegressor(**nn_params)\n",
    "nn_model.fit(X_train_scaled, y_train_split)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_nn = nn_model.predict(X_train_scaled)\n",
    "y_val_pred_nn = nn_model.predict(X_val_scaled)\n",
    "y_test_pred_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "nn_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train_split, y_train_pred_nn)),\n",
    "    'train_mae': mean_absolute_error(y_train_split, y_train_pred_nn),\n",
    "    'train_r2': r2_score(y_train_split, y_train_pred_nn),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred_nn)),\n",
    "    'val_mae': mean_absolute_error(y_val, y_val_pred_nn),\n",
    "    'val_r2': r2_score(y_val, y_val_pred_nn),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred_nn)),\n",
    "    'test_mae': mean_absolute_error(y_test, y_test_pred_nn),\n",
    "    'test_r2': r2_score(y_test, y_test_pred_nn)\n",
    "}\n",
    "\n",
    "print(\"Neural Network Results:\")\n",
    "for metric, value in nn_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(nn_model, os.path.join(MODEL_DIR, 'neural_network_model.pkl'))\n",
    "joblib.dump(nn_scaler, os.path.join(MODEL_DIR, 'nn_scaler.pkl'))\n",
    "print(\"\\nNeural Network model and scaler saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': ['XGBoost', 'Random Forest', 'Neural Network'],\n",
    "    'Train_RMSE': [xgb_metrics['train_rmse'], rf_metrics['train_rmse'], nn_metrics['train_rmse']],\n",
    "    'Val_RMSE': [xgb_metrics['val_rmse'], rf_metrics['val_rmse'], nn_metrics['val_rmse']],\n",
    "    'Test_RMSE': [xgb_metrics['test_rmse'], rf_metrics['test_rmse'], nn_metrics['test_rmse']],\n",
    "    'Train_MAE': [xgb_metrics['train_mae'], rf_metrics['train_mae'], nn_metrics['train_mae']],\n",
    "    'Val_MAE': [xgb_metrics['val_mae'], rf_metrics['val_mae'], nn_metrics['val_mae']],\n",
    "    'Test_MAE': [xgb_metrics['test_mae'], rf_metrics['test_mae'], nn_metrics['test_mae']],\n",
    "    'Train_R2': [xgb_metrics['train_r2'], rf_metrics['train_r2'], nn_metrics['train_r2']],\n",
    "    'Val_R2': [xgb_metrics['val_r2'], rf_metrics['val_r2'], nn_metrics['val_r2']],\n",
    "    'Test_R2': [xgb_metrics['test_r2'], rf_metrics['test_r2'], nn_metrics['test_r2']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(os.path.join(MODEL_DIR, 'model_comparison.csv'), index=False)\n",
    "\n",
    "# Find best model based on validation RMSE\n",
    "best_model_idx = comparison_df['Val_RMSE'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nBest model based on validation RMSE: {best_model_name}\")\n",
    "print(f\"Validation RMSE: {comparison_df.loc[best_model_idx, 'Val_RMSE']:.4f}\")\n",
    "print(f\"Test RMSE: {comparison_df.loc[best_model_idx, 'Test_RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# XGBoost importance\n",
    "top_features_xgb = xgb_importance.head(15)\n",
    "axes[0].barh(range(len(top_features_xgb)), top_features_xgb['importance'])\n",
    "axes[0].set_yticks(range(len(top_features_xgb)))\n",
    "axes[0].set_yticklabels(top_features_xgb['feature'])\n",
    "axes[0].set_title('XGBoost Feature Importance (Top 15)')\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "# Random Forest importance\n",
    "top_features_rf = rf_importance.head(15)\n",
    "axes[1].barh(range(len(top_features_rf)), top_features_rf['importance'])\n",
    "axes[1].set_yticks(range(len(top_features_rf)))\n",
    "axes[1].set_yticklabels(top_features_rf['feature'])\n",
    "axes[1].set_title('Random Forest Feature Importance (Top 15)')\n",
    "axes[1].set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance\n",
    "xgb_importance.to_csv(os.path.join(MODEL_DIR, 'xgb_feature_importance.csv'), index=False)\n",
    "rf_importance.to_csv(os.path.join(MODEL_DIR, 'rf_feature_importance.csv'), index=False)\n",
    "\n",
    "print(\"Top 10 features (XGBoost):\")\n",
    "print(xgb_importance.head(10))\n",
    "\n",
    "print(\"\\nTop 10 features (Random Forest):\")\n",
    "print(rf_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series cross-validation\n",
    "print(\"Performing time series cross-validation...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# XGBoost CV\n",
    "xgb_cv_scores = cross_val_score(\n",
    "    xgb.XGBRegressor(**xgb_params), \n",
    "    X_train, y_train, \n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_cv_rmse = np.sqrt(-xgb_cv_scores)\n",
    "\n",
    "# Random Forest CV\n",
    "rf_cv_scores = cross_val_score(\n",
    "    RandomForestRegressor(**rf_params), \n",
    "    X_train, y_train, \n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_cv_rmse = np.sqrt(-rf_cv_scores)\n",
    "\n",
    "# Neural Network CV (scaled data)\n",
    "X_train_scaled_full = nn_scaler.fit_transform(X_train)\n",
    "nn_cv_scores = cross_val_score(\n",
    "    MLPRegressor(**nn_params), \n",
    "    X_train_scaled_full, y_train, \n",
    "    cv=tscv, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "nn_cv_rmse = np.sqrt(-nn_cv_scores)\n",
    "\n",
    "# CV Results\n",
    "cv_results = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'Random Forest', 'Neural Network'],\n",
    "    'CV_RMSE_Mean': [xgb_cv_rmse.mean(), rf_cv_rmse.mean(), nn_cv_rmse.mean()],\n",
    "    'CV_RMSE_Std': [xgb_cv_rmse.std(), rf_cv_rmse.std(), nn_cv_rmse.std()]\n",
    "})\n",
    "\n",
    "print(\"\\n=== CROSS-VALIDATION RESULTS ===\")\n",
    "print(cv_results.round(4))\n",
    "\n",
    "# Save CV results\n",
    "cv_results.to_csv(os.path.join(MODEL_DIR, 'cv_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_info': {\n",
    "        'total_samples': len(data_clean),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'features': len(feature_cols),\n",
    "        'target': target_col,\n",
    "        'date_range': [str(data_clean['Time'].min()), str(data_clean['Time'].max())],\n",
    "        'stations': list(data_clean['station'].unique())\n",
    "    },\n",
    "    'models': {\n",
    "        'xgboost': {\n",
    "            'parameters': xgb_params,\n",
    "            'metrics': xgb_metrics,\n",
    "            'cv_rmse_mean': float(xgb_cv_rmse.mean()),\n",
    "            'cv_rmse_std': float(xgb_cv_rmse.std()),\n",
    "            'model_file': 'xgboost_model.pkl'\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'parameters': rf_params,\n",
    "            'metrics': rf_metrics,\n",
    "            'cv_rmse_mean': float(rf_cv_rmse.mean()),\n",
    "            'cv_rmse_std': float(rf_cv_rmse.std()),\n",
    "            'model_file': 'random_forest_model.pkl'\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'parameters': nn_params,\n",
    "            'metrics': nn_metrics,\n",
    "            'cv_rmse_mean': float(nn_cv_rmse.mean()),\n",
    "            'cv_rmse_std': float(nn_cv_rmse.std()),\n",
    "            'model_file': 'neural_network_model.pkl',\n",
    "            'scaler_file': 'nn_scaler.pkl'\n",
    "        }\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'val_rmse': float(comparison_df.loc[best_model_idx, 'Val_RMSE']),\n",
    "        'test_rmse': float(comparison_df.loc[best_model_idx, 'Test_RMSE'])\n",
    "    },\n",
    "    'feature_columns': feature_cols\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(os.path.join(MODEL_DIR, 'model_metadata.json'), 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n=== MODEL TRAINING COMPLETE ===\")\n",
    "print(f\"Models saved to: {MODEL_DIR}\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best validation RMSE: {comparison_df.loc[best_model_idx, 'Val_RMSE']:.4f}\")\n",
    "print(f\"Best test RMSE: {comparison_df.loc[best_model_idx, 'Test_RMSE']:.4f}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"- xgboost_model.pkl\")\n",
    "print(f\"- random_forest_model.pkl\")\n",
    "print(f\"- neural_network_model.pkl\")\n",
    "print(f\"- nn_scaler.pkl\")\n",
    "print(f\"- model_metadata.json\")\n",
    "print(f\"- model_comparison.csv\")\n",
    "print(f\"- cv_results.csv\")\n",
    "print(f\"- feature_importance.png\")\n",
    "print(f\"- xgb_feature_importance.csv\")\n",
    "print(f\"- rf_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

