{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Deployment for Solar Power Generation ML Pipeline\n",
    "\n",
    "This notebook demonstrates how to deploy the trained solar power generation model to Amazon SageMaker for production inference.\n",
    "\n",
    "## Overview\n",
    "- Deploy trained model to SageMaker endpoint\n",
    "- Test real-time inference\n",
    "- Set up batch transform jobs\n",
    "- Configure auto-scaling and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"AWS region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'solar-power-ml'\n",
    "model_name = 'solar-power-neural-network'\n",
    "endpoint_name = f'solar-power-endpoint-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Model prefix: {prefix}\")\n",
    "print(f\"Endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and preprocessor\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Load model artifacts\n",
    "model_path = '../models/trained/neural_network_model.pkl'\n",
    "scaler_path = '../models/trained/scaler.pkl'\n",
    "\n",
    "if os.path.exists(model_path) and os.path.exists(scaler_path):\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(\"✅ Model and scaler loaded successfully\")\n",
    "else:\n",
    "    print(\"❌ Model files not found. Please run the training notebook first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model artifacts directory\n",
    "os.makedirs('../sagemaker_deployment/model_artifacts', exist_ok=True)\n",
    "\n",
    "# Copy model files to deployment directory\n",
    "import shutil\n",
    "shutil.copy(model_path, '../sagemaker_deployment/model_artifacts/')\n",
    "shutil.copy(scaler_path, '../sagemaker_deployment/model_artifacts/')\n",
    "\n",
    "print(\"Model artifacts prepared for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create SageMaker Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../sagemaker_deployment/code/train.py\n",
    "\"\"\"\n",
    "SageMaker Training Script for Solar Power Generation Model\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import json\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model for inference\"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, 'model.pkl'))\n",
    "    scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))\n",
    "    return {'model': model, 'scaler': scaler}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data\"\"\"\n",
    "    if request_content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)\n",
    "        return pd.DataFrame(input_data)\n",
    "    elif request_content_type == 'text/csv':\n",
    "        return pd.read_csv(request_body)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    model = model_dict['model']\n",
    "    scaler = model_dict['scaler']\n",
    "    \n",
    "    # Scale input data\n",
    "    scaled_data = scaler.transform(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(scaled_data)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"Format output\"\"\"\n",
    "    if content_type == 'application/json':\n",
    "        return json.dumps(prediction.tolist())\n",
    "    elif content_type == 'text/csv':\n",
    "        return ','.join(map(str, prediction))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Hyperparameters\n",
    "    parser.add_argument('--hidden_layer_sizes', type=str, default='100,50,25')\n",
    "    parser.add_argument('--alpha', type=float, default=0.001)\n",
    "    parser.add_argument('--max_iter', type=int, default=300)\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    \n",
    "    # SageMaker specific arguments\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load training data\n",
    "    train_data = pd.read_csv(os.path.join(args.train, 'train.csv'))\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_columns = [col for col in train_data.columns if col != 'generation']\n",
    "    X = train_data[feature_columns]\n",
    "    y = train_data['generation']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=args.random_state\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Parse hidden layer sizes\n",
    "    hidden_layers = tuple(map(int, args.hidden_layer_sizes.split(',')))\n",
    "    \n",
    "    # Train model\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        alpha=args.alpha,\n",
    "        max_iter=args.max_iter,\n",
    "        random_state=args.random_state,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    \n",
    "    # Save model and scaler\n",
    "    joblib.dump(model, os.path.join(args.model_dir, 'model.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(args.model_dir, 'scaler.pkl'))\n",
    "    \n",
    "    # Save feature names\n",
    "    with open(os.path.join(args.model_dir, 'feature_names.json'), 'w') as f:\n",
    "        json.dump(feature_columns, f)\n",
    "    \n",
    "    # Save model metrics\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'r2_score': r2,\n",
    "        'mae': mae,\n",
    "        'n_features': len(feature_columns),\n",
    "        'n_samples': len(train_data)\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(args.model_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    print(\"Model training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create SageMaker Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../sagemaker_deployment/code/inference.py\n",
    "\"\"\"\n",
    "SageMaker Inference Script for Solar Power Generation Model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model and preprocessor for inference\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = joblib.load(os.path.join(model_dir, 'model.pkl'))\n",
    "        scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))\n",
    "        \n",
    "        # Load feature names if available\n",
    "        feature_names_path = os.path.join(model_dir, 'feature_names.json')\n",
    "        if os.path.exists(feature_names_path):\n",
    "            with open(feature_names_path, 'r') as f:\n",
    "                feature_names = json.load(f)\n",
    "        else:\n",
    "            feature_names = None\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Parse and preprocess the input data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if request_content_type == 'application/json':\n",
    "            input_data = json.loads(request_body)\n",
    "            \n",
    "            # Handle different JSON formats\n",
    "            if isinstance(input_data, dict):\n",
    "                if 'instances' in input_data:\n",
    "                    # SageMaker batch format\n",
    "                    df = pd.DataFrame(input_data['instances'])\n",
    "                else:\n",
    "                    # Single instance format\n",
    "                    df = pd.DataFrame([input_data])\n",
    "            elif isinstance(input_data, list):\n",
    "                # List of instances\n",
    "                df = pd.DataFrame(input_data)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid JSON format\")\n",
    "                \n",
    "        elif request_content_type == 'text/csv':\n",
    "            # CSV format\n",
    "            df = pd.read_csv(StringIO(request_body))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error parsing input: {str(e)}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model_dict['model']\n",
    "        scaler = model_dict['scaler']\n",
    "        feature_names = model_dict.get('feature_names')\n",
    "        \n",
    "        # Ensure correct feature order if feature names are available\n",
    "        if feature_names:\n",
    "            # Check if all required features are present\n",
    "            missing_features = set(feature_names) - set(input_data.columns)\n",
    "            if missing_features:\n",
    "                raise ValueError(f\"Missing features: {missing_features}\")\n",
    "            \n",
    "            # Reorder columns to match training data\n",
    "            input_data = input_data[feature_names]\n",
    "        \n",
    "        # Scale the input data\n",
    "        scaled_data = scaler.transform(input_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(scaled_data)\n",
    "        \n",
    "        # Ensure predictions are non-negative (solar generation can't be negative)\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error making predictions: {str(e)}\")\n",
    "\n",
    "def output_fn(predictions, content_type):\n",
    "    \"\"\"\n",
    "    Format the predictions for output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if content_type == 'application/json':\n",
    "            # Return predictions with metadata\n",
    "            output = {\n",
    "                'predictions': predictions.tolist(),\n",
    "                'model_name': 'solar_power_neural_network',\n",
    "                'prediction_count': len(predictions),\n",
    "                'units': 'kWh'\n",
    "            }\n",
    "            return json.dumps(output)\n",
    "            \n",
    "        elif content_type == 'text/csv':\n",
    "            # Return as CSV\n",
    "            output_df = pd.DataFrame({\n",
    "                'predicted_generation_kwh': predictions\n",
    "            })\n",
    "            return output_df.to_csv(index=False)\n",
    "            \n",
    "        else:\n",
    "            # Default: return as plain text\n",
    "            return ','.join(map(str, predictions))\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error formatting output: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Training Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload processed training data to S3\n",
    "train_data_path = '../data/processed/processed_solar_data.csv'\n",
    "\n",
    "if os.path.exists(train_data_path):\n",
    "    # Upload to S3\n",
    "    train_s3_path = sagemaker_session.upload_data(\n",
    "        path=train_data_path,\n",
    "        bucket=bucket,\n",
    "        key_prefix=f'{prefix}/data/train'\n",
    "    )\n",
    "    print(f\"Training data uploaded to: {train_s3_path}\")\n",
    "else:\n",
    "    print(\"❌ Training data not found. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create and Train SageMaker Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker estimator\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    source_dir='../sagemaker_deployment/code',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.large',\n",
    "    framework_version='0.23-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'hidden_layer_sizes': '100,50,25',\n",
    "        'alpha': 0.001,\n",
    "        'max_iter': 300,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    output_path=f's3://{bucket}/{prefix}/model-artifacts',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(\"SageMaker estimator created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training job\n",
    "print(\"Starting SageMaker training job...\")\n",
    "sklearn_estimator.fit({'train': train_s3_path})\n",
    "print(\"Training job completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to endpoint\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "\n",
    "predictor = sklearn_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"✅ Model deployed successfully to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Real-time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for inference\n",
    "test_data_path = '../data/processed/processed_solar_data.csv'\n",
    "\n",
    "if os.path.exists(test_data_path):\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Get feature columns (exclude target)\n",
    "    feature_columns = [col for col in test_df.columns if col != 'generation']\n",
    "    \n",
    "    # Select a few samples for testing\n",
    "    test_samples = test_df[feature_columns].head(5)\n",
    "    actual_values = test_df['generation'].head(5)\n",
    "    \n",
    "    print(\"Test samples prepared:\")\n",
    "    print(test_samples)\n",
    "else:\n",
    "    print(\"❌ Test data not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "try:\n",
    "    # Convert to JSON format\n",
    "    test_input = test_samples.to_dict('records')\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = predictor.predict(test_input)\n",
    "    \n",
    "    # Parse predictions\n",
    "    if isinstance(predictions, str):\n",
    "        pred_data = json.loads(predictions)\n",
    "        predicted_values = pred_data['predictions']\n",
    "    else:\n",
    "        predicted_values = predictions\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': actual_values.values,\n",
    "        'Predicted': predicted_values,\n",
    "        'Difference': actual_values.values - predicted_values\n",
    "    })\n",
    "    \n",
    "    print(\"\\n🎯 Prediction Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((actual_values.values - predicted_values) ** 2))\n",
    "    mae = np.mean(np.abs(actual_values.values - predicted_values))\n",
    "    \n",
    "    print(f\"\\n📊 Test Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.4f} kWh\")\n",
    "    print(f\"MAE: {mae:.4f} kWh\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error making predictions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Set up Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch transform job for large-scale predictions\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Create transformer\n",
    "transformer = Transformer(\n",
    "    model_name=sklearn_estimator.model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=f's3://{bucket}/{prefix}/batch-predictions',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(\"Batch transformer created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch data (optional - for demonstration)\n",
    "batch_data_path = '../data/processed/batch_test_data.csv'\n",
    "\n",
    "# Create a sample batch file\n",
    "if os.path.exists(test_data_path):\n",
    "    batch_df = test_df[feature_columns].head(100)  # First 100 samples\n",
    "    batch_df.to_csv(batch_data_path, index=False)\n",
    "    \n",
    "    # Upload batch data to S3\n",
    "    batch_s3_path = sagemaker_session.upload_data(\n",
    "        path=batch_data_path,\n",
    "        bucket=bucket,\n",
    "        key_prefix=f'{prefix}/data/batch'\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch data uploaded to: {batch_s3_path}\")\n",
    "    \n",
    "    # Run batch transform (uncomment to execute)\n",
    "    # transformer.transform(\n",
    "    #     data=batch_s3_path,\n",
    "    #     content_type='text/csv',\n",
    "    #     split_type='Line'\n",
    "    # )\n",
    "    # print(\"Batch transform job started\")\n",
    "else:\n",
    "    print(\"Test data not available for batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitor Endpoint Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CloudWatch monitoring\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "def get_endpoint_metrics(endpoint_name, start_time, end_time):\n",
    "    \"\"\"Get endpoint metrics from CloudWatch\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'Invocations': [],\n",
    "        'ModelLatency': [],\n",
    "        'OverheadLatency': []\n",
    "    }\n",
    "    \n",
    "    for metric_name in metrics.keys():\n",
    "        try:\n",
    "            response = cloudwatch.get_metric_statistics(\n",
    "                Namespace='AWS/SageMaker',\n",
    "                MetricName=metric_name,\n",
    "                Dimensions=[\n",
    "                    {\n",
    "                        'Name': 'EndpointName',\n",
    "                        'Value': endpoint_name\n",
    "                    },\n",
    "                ],\n",
    "                StartTime=start_time,\n",
    "                EndTime=end_time,\n",
    "                Period=300,  # 5 minutes\n",
    "                Statistics=['Average', 'Sum']\n",
    "            )\n",
    "            metrics[metric_name] = response['Datapoints']\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting {metric_name}: {e}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Get metrics for the last hour\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(hours=1)\n",
    "\n",
    "metrics = get_endpoint_metrics(endpoint_name, start_time, end_time)\n",
    "print(f\"📊 Endpoint metrics for {endpoint_name}:\")\n",
    "for metric_name, datapoints in metrics.items():\n",
    "    if datapoints:\n",
    "        latest = datapoints[-1]\n",
    "        print(f\"{metric_name}: {latest.get('Average', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"{metric_name}: No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Auto-scaling Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure auto-scaling for the endpoint\n",
    "autoscaling_client = boto3.client('application-autoscaling')\n",
    "\n",
    "# Register scalable target\n",
    "resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic'\n",
    "\n",
    "try:\n",
    "    autoscaling_client.register_scalable_target(\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "        MinCapacity=1,\n",
    "        MaxCapacity=5\n",
    "    )\n",
    "    \n",
    "    # Create scaling policy\n",
    "    autoscaling_client.put_scaling_policy(\n",
    "        PolicyName=f'{endpoint_name}-scaling-policy',\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "        PolicyType='TargetTrackingScaling',\n",
    "        TargetTrackingScalingPolicyConfiguration={\n",
    "            'TargetValue': 70.0,\n",
    "            'PredefinedMetricSpecification': {\n",
    "                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n",
    "            },\n",
    "            'ScaleOutCooldown': 300,\n",
    "            'ScaleInCooldown': 300\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Auto-scaling configured successfully\")\n",
    "    print(\"- Min instances: 1\")\n",
    "    print(\"- Max instances: 5\")\n",
    "    print(\"- Target: 70 invocations per instance\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Auto-scaling setup failed: {e}\")\n",
    "    print(\"This is normal if auto-scaling is already configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the endpoint (to avoid charges)\n",
    "# predictor.delete_endpoint()\n",
    "# print(f\"Endpoint {endpoint_name} deleted\")\n",
    "\n",
    "print(\"\\n⚠️ Remember to delete the endpoint when no longer needed to avoid charges:\")\n",
    "print(f\"predictor.delete_endpoint()\")\n",
    "print(f\"\\nOr use AWS CLI:\")\n",
    "print(f\"aws sagemaker delete-endpoint --endpoint-name {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Model Deployment**: Successfully deployed the trained neural network model to SageMaker\n",
    "2. **Real-time Inference**: Tested the endpoint with sample data\n",
    "3. **Batch Processing**: Set up batch transform for large-scale predictions\n",
    "4. **Monitoring**: Configured CloudWatch monitoring for endpoint performance\n",
    "5. **Auto-scaling**: Set up automatic scaling based on traffic\n",
    "\n",
    "### Key Features:\n",
    "- **Production Ready**: Scalable endpoint with monitoring\n",
    "- **Cost Optimized**: Auto-scaling to handle variable load\n",
    "- **Robust**: Error handling and validation\n",
    "- **Flexible**: Supports both JSON and CSV input formats\n",
    "\n",
    "### Next Steps:\n",
    "1. Set up automated retraining pipeline\n",
    "2. Implement A/B testing for model updates\n",
    "3. Add custom metrics and alarms\n",
    "4. Integrate with application APIs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

