{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitoring and Drift Detection for Solar Power Generation ML\n",
    "\n",
    "This notebook demonstrates comprehensive model monitoring, including data drift detection, performance tracking, and automated alerting for the solar power generation ML pipeline.\n",
    "\n",
    "## Overview\n",
    "- Monitor model performance in production\n",
    "- Detect data and model drift\n",
    "- Track key performance indicators\n",
    "- Set up automated alerts\n",
    "- Generate monitoring reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📊 Model monitoring environment initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"solar-power-neural-network\"\n",
    "endpoint_name = \"solar-power-endpoint\"  # Replace with your endpoint name\n",
    "baseline_data_path = \"../data/baseline/baseline_data.csv\"\n",
    "monitoring_bucket = \"your-monitoring-bucket\"  # Replace with your bucket\n",
    "\n",
    "# Drift detection thresholds\n",
    "DRIFT_THRESHOLD = 0.05  # p-value threshold for statistical tests\n",
    "PERFORMANCE_THRESHOLD = 0.10  # 10% degradation threshold\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Drift threshold: {DRIFT_THRESHOLD}\")\n",
    "print(f\"Performance threshold: {PERFORMANCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDriftDetector:\n",
    "    \"\"\"\n",
    "    Comprehensive data drift detection using statistical tests\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_data, threshold=0.05):\n",
    "        self.baseline_data = baseline_data\n",
    "        self.threshold = threshold\n",
    "        self.drift_results = {}\n",
    "    \n",
    "    def kolmogorov_smirnov_test(self, baseline_feature, current_feature):\n",
    "        \"\"\"\n",
    "        Perform Kolmogorov-Smirnov test for distribution comparison\n",
    "        \"\"\"\n",
    "        statistic, p_value = stats.ks_2samp(baseline_feature, current_feature)\n",
    "        return {\n",
    "            'test': 'kolmogorov_smirnov',\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < self.threshold\n",
    "        }\n",
    "    \n",
    "    def chi_square_test(self, baseline_feature, current_feature, bins=10):\n",
    "        \"\"\"\n",
    "        Perform Chi-square test for categorical or binned continuous features\n",
    "        \"\"\"\n",
    "        # Create bins for continuous features\n",
    "        min_val = min(baseline_feature.min(), current_feature.min())\n",
    "        max_val = max(baseline_feature.max(), current_feature.max())\n",
    "        bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        \n",
    "        # Get histograms\n",
    "        baseline_hist, _ = np.histogram(baseline_feature, bins=bin_edges)\n",
    "        current_hist, _ = np.histogram(current_feature, bins=bin_edges)\n",
    "        \n",
    "        # Avoid zero frequencies\n",
    "        baseline_hist = baseline_hist + 1\n",
    "        current_hist = current_hist + 1\n",
    "        \n",
    "        # Perform chi-square test\n",
    "        statistic, p_value = stats.chisquare(current_hist, baseline_hist)\n",
    "        \n",
    "        return {\n",
    "            'test': 'chi_square',\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < self.threshold\n",
    "        }\n",
    "    \n",
    "    def population_stability_index(self, baseline_feature, current_feature, bins=10):\n",
    "        \"\"\"\n",
    "        Calculate Population Stability Index (PSI)\n",
    "        \"\"\"\n",
    "        # Create bins\n",
    "        min_val = baseline_feature.min()\n",
    "        max_val = baseline_feature.max()\n",
    "        bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        \n",
    "        # Calculate proportions\n",
    "        baseline_props = np.histogram(baseline_feature, bins=bin_edges)[0] / len(baseline_feature)\n",
    "        current_props = np.histogram(current_feature, bins=bin_edges)[0] / len(current_feature)\n",
    "        \n",
    "        # Avoid zero proportions\n",
    "        baseline_props = np.where(baseline_props == 0, 0.0001, baseline_props)\n",
    "        current_props = np.where(current_props == 0, 0.0001, current_props)\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = np.sum((current_props - baseline_props) * np.log(current_props / baseline_props))\n",
    "        \n",
    "        # PSI interpretation: <0.1 (no drift), 0.1-0.2 (moderate), >0.2 (significant)\n",
    "        drift_detected = psi > 0.1\n",
    "        \n",
    "        return {\n",
    "            'test': 'population_stability_index',\n",
    "            'psi_value': psi,\n",
    "            'drift_detected': drift_detected,\n",
    "            'interpretation': 'significant' if psi > 0.2 else 'moderate' if psi > 0.1 else 'stable'\n",
    "        }\n",
    "    \n",
    "    def detect_drift(self, current_data, feature_columns=None):\n",
    "        \"\"\"\n",
    "        Detect drift across all features\n",
    "        \"\"\"\n",
    "        if feature_columns is None:\n",
    "            feature_columns = [col for col in self.baseline_data.columns if col != 'generation']\n",
    "        \n",
    "        drift_results = {}\n",
    "        \n",
    "        for feature in feature_columns:\n",
    "            if feature in current_data.columns:\n",
    "                baseline_feature = self.baseline_data[feature].dropna()\n",
    "                current_feature = current_data[feature].dropna()\n",
    "                \n",
    "                # Perform multiple tests\n",
    "                ks_result = self.kolmogorov_smirnov_test(baseline_feature, current_feature)\n",
    "                chi2_result = self.chi_square_test(baseline_feature, current_feature)\n",
    "                psi_result = self.population_stability_index(baseline_feature, current_feature)\n",
    "                \n",
    "                # Statistical summary\n",
    "                baseline_stats = {\n",
    "                    'mean': baseline_feature.mean(),\n",
    "                    'std': baseline_feature.std(),\n",
    "                    'min': baseline_feature.min(),\n",
    "                    'max': baseline_feature.max()\n",
    "                }\n",
    "                \n",
    "                current_stats = {\n",
    "                    'mean': current_feature.mean(),\n",
    "                    'std': current_feature.std(),\n",
    "                    'min': current_feature.min(),\n",
    "                    'max': current_feature.max()\n",
    "                }\n",
    "                \n",
    "                drift_results[feature] = {\n",
    "                    'kolmogorov_smirnov': ks_result,\n",
    "                    'chi_square': chi2_result,\n",
    "                    'population_stability_index': psi_result,\n",
    "                    'baseline_stats': baseline_stats,\n",
    "                    'current_stats': current_stats,\n",
    "                    'overall_drift': any([\n",
    "                        ks_result['drift_detected'],\n",
    "                        chi2_result['drift_detected'],\n",
    "                        psi_result['drift_detected']\n",
    "                    ])\n",
    "                }\n",
    "        \n",
    "        self.drift_results = drift_results\n",
    "        return drift_results\n",
    "    \n",
    "    def generate_drift_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive drift detection report\n",
    "        \"\"\"\n",
    "        if not self.drift_results:\n",
    "            return \"No drift analysis performed yet. Run detect_drift() first.\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"DATA DRIFT DETECTION REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(f\"Drift Threshold: {self.threshold}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Summary\n",
    "        total_features = len(self.drift_results)\n",
    "        drifted_features = sum(1 for result in self.drift_results.values() if result['overall_drift'])\n",
    "        \n",
    "        report.append(\"SUMMARY:\")\n",
    "        report.append(f\"Total Features Analyzed: {total_features}\")\n",
    "        report.append(f\"Features with Drift: {drifted_features}\")\n",
    "        report.append(f\"Drift Percentage: {(drifted_features/total_features)*100:.1f}%\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Detailed results\n",
    "        report.append(\"DETAILED RESULTS:\")\n",
    "        report.append(\"-\" * 40)\n",
    "        \n",
    "        for feature, results in self.drift_results.items():\n",
    "            report.append(f\"\\nFeature: {feature}\")\n",
    "            report.append(f\"Overall Drift: {'YES' if results['overall_drift'] else 'NO'}\")\n",
    "            \n",
    "            # Test results\n",
    "            ks = results['kolmogorov_smirnov']\n",
    "            report.append(f\"  KS Test: p-value={ks['p_value']:.4f}, drift={'YES' if ks['drift_detected'] else 'NO'}\")\n",
    "            \n",
    "            chi2 = results['chi_square']\n",
    "            report.append(f\"  Chi² Test: p-value={chi2['p_value']:.4f}, drift={'YES' if chi2['drift_detected'] else 'NO'}\")\n",
    "            \n",
    "            psi = results['population_stability_index']\n",
    "            report.append(f\"  PSI: {psi['psi_value']:.4f} ({psi['interpretation']})\")\n",
    "            \n",
    "            # Statistical changes\n",
    "            baseline = results['baseline_stats']\n",
    "            current = results['current_stats']\n",
    "            mean_change = ((current['mean'] - baseline['mean']) / baseline['mean']) * 100\n",
    "            std_change = ((current['std'] - baseline['std']) / baseline['std']) * 100\n",
    "            \n",
    "            report.append(f\"  Mean Change: {mean_change:+.2f}%\")\n",
    "            report.append(f\"  Std Change: {std_change:+.2f}%\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "print(\"✅ DataDriftDetector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor model performance over time\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_metrics, threshold=0.10):\n",
    "        self.baseline_metrics = baseline_metrics\n",
    "        self.threshold = threshold\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive performance metrics\n",
    "        \"\"\"\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        \n",
    "        # Additional metrics\n",
    "        residuals = y_true - y_pred\n",
    "        residual_std = np.std(residuals)\n",
    "        residual_mean = np.mean(residuals)\n",
    "        \n",
    "        # Prediction intervals\n",
    "        prediction_std = np.std(y_pred)\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'r2_score': r2,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'residual_std': residual_std,\n",
    "            'residual_mean': residual_mean,\n",
    "            'prediction_std': prediction_std,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def detect_performance_degradation(self, current_metrics):\n",
    "        \"\"\"\n",
    "        Detect if model performance has degraded\n",
    "        \"\"\"\n",
    "        degradation_flags = {}\n",
    "        \n",
    "        # Check RMSE increase\n",
    "        rmse_increase = (current_metrics['rmse'] - self.baseline_metrics['rmse']) / self.baseline_metrics['rmse']\n",
    "        degradation_flags['rmse_degraded'] = rmse_increase > self.threshold\n",
    "        \n",
    "        # Check R² decrease\n",
    "        r2_decrease = (self.baseline_metrics['r2_score'] - current_metrics['r2_score']) / self.baseline_metrics['r2_score']\n",
    "        degradation_flags['r2_degraded'] = r2_decrease > self.threshold\n",
    "        \n",
    "        # Check MAE increase\n",
    "        mae_increase = (current_metrics['mae'] - self.baseline_metrics['mae']) / self.baseline_metrics['mae']\n",
    "        degradation_flags['mae_degraded'] = mae_increase > self.threshold\n",
    "        \n",
    "        # Overall degradation\n",
    "        degradation_flags['overall_degraded'] = any([\n",
    "            degradation_flags['rmse_degraded'],\n",
    "            degradation_flags['r2_degraded'],\n",
    "            degradation_flags['mae_degraded']\n",
    "        ])\n",
    "        \n",
    "        # Calculate percentage changes\n",
    "        degradation_flags['rmse_change_pct'] = rmse_increase * 100\n",
    "        degradation_flags['r2_change_pct'] = -r2_decrease * 100\n",
    "        degradation_flags['mae_change_pct'] = mae_increase * 100\n",
    "        \n",
    "        return degradation_flags\n",
    "    \n",
    "    def add_performance_record(self, metrics):\n",
    "        \"\"\"\n",
    "        Add performance record to history\n",
    "        \"\"\"\n",
    "        self.performance_history.append(metrics)\n",
    "    \n",
    "    def plot_performance_trends(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot performance trends over time\n",
    "        \"\"\"\n",
    "        if not self.performance_history:\n",
    "            print(\"No performance history available\")\n",
    "            return\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(self.performance_history)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Model Performance Trends', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # RMSE trend\n",
    "        axes[0, 0].plot(df['timestamp'], df['rmse'], marker='o', linewidth=2)\n",
    "        axes[0, 0].axhline(y=self.baseline_metrics['rmse'], color='red', linestyle='--', label='Baseline')\n",
    "        axes[0, 0].set_title('RMSE Trend')\n",
    "        axes[0, 0].set_ylabel('RMSE')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # R² trend\n",
    "        axes[0, 1].plot(df['timestamp'], df['r2_score'], marker='o', linewidth=2, color='green')\n",
    "        axes[0, 1].axhline(y=self.baseline_metrics['r2_score'], color='red', linestyle='--', label='Baseline')\n",
    "        axes[0, 1].set_title('R² Score Trend')\n",
    "        axes[0, 1].set_ylabel('R² Score')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE trend\n",
    "        axes[1, 0].plot(df['timestamp'], df['mae'], marker='o', linewidth=2, color='orange')\n",
    "        axes[1, 0].axhline(y=self.baseline_metrics['mae'], color='red', linestyle='--', label='Baseline')\n",
    "        axes[1, 0].set_title('MAE Trend')\n",
    "        axes[1, 0].set_ylabel('MAE')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAPE trend\n",
    "        axes[1, 1].plot(df['timestamp'], df['mape'], marker='o', linewidth=2, color='purple')\n",
    "        axes[1, 1].set_title('MAPE Trend')\n",
    "        axes[1, 1].set_ylabel('MAPE (%)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Performance trends plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ ModelPerformanceMonitor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Baseline Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline data (training data used for model development)\n",
    "try:\n",
    "    baseline_data = pd.read_csv('../data/processed/processed_solar_data.csv')\n",
    "    print(f\"✅ Baseline data loaded: {len(baseline_data)} samples\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\n📊 Baseline Data Summary:\")\n",
    "    print(baseline_data.describe())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Baseline data not found. Please run preprocessing first.\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    baseline_data = pd.DataFrame({\n",
    "        'generation': np.random.normal(2.5, 1.0, 1000),\n",
    "        'irradiance': np.random.normal(500, 200, 1000),\n",
    "        'temperature': np.random.normal(25, 5, 1000),\n",
    "        'humidity': np.random.normal(60, 15, 1000),\n",
    "        'hour': np.random.randint(0, 24, 1000)\n",
    "    })\n",
    "    print(\"📝 Using sample baseline data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline performance metrics (from model training)\n",
    "baseline_metrics = {\n",
    "    'rmse': 0.2935,\n",
    "    'r2_score': 0.9905,\n",
    "    'mae': 0.1142,\n",
    "    'mape': 8.2\n",
    "}\n",
    "\n",
    "print(\"📊 Baseline Performance Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric.upper()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulate Current Data and Detect Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate current data with some drift\n",
    "np.random.seed(123)\n",
    "n_current = 500\n",
    "\n",
    "# Create current data with intentional drift in some features\n",
    "current_data = baseline_data.sample(n_current).copy()\n",
    "\n",
    "# Introduce drift in temperature (shift mean by 2 degrees)\n",
    "if 'temperature' in current_data.columns:\n",
    "    current_data['temperature'] = current_data['temperature'] + 2.0\n",
    "\n",
    "# Introduce drift in irradiance (increase variance)\n",
    "if 'irradiance' in current_data.columns:\n",
    "    noise = np.random.normal(0, 50, len(current_data))\n",
    "    current_data['irradiance'] = current_data['irradiance'] + noise\n",
    "\n",
    "# Introduce slight drift in humidity\n",
    "if 'humidity' in current_data.columns:\n",
    "    current_data['humidity'] = current_data['humidity'] * 1.1\n",
    "\n",
    "print(f\"📊 Current data simulated: {len(current_data)} samples\")\n",
    "print(\"\\n🔄 Intentional drift introduced:\")\n",
    "print(\"  - Temperature: +2°C mean shift\")\n",
    "print(\"  - Irradiance: Increased variance\")\n",
    "print(\"  - Humidity: 10% multiplicative increase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drift detector\n",
    "drift_detector = DataDriftDetector(baseline_data, threshold=DRIFT_THRESHOLD)\n",
    "\n",
    "# Detect drift\n",
    "print(\"🔍 Running drift detection analysis...\")\n",
    "drift_results = drift_detector.detect_drift(current_data)\n",
    "\n",
    "# Generate and display report\n",
    "drift_report = drift_detector.generate_drift_report()\n",
    "print(drift_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Drift Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(baseline_data, current_data, feature_columns, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot distribution comparisons for drift visualization\n",
    "    \"\"\"\n",
    "    n_features = len(feature_columns)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    fig.suptitle('Feature Distribution Comparison: Baseline vs Current', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, feature in enumerate(feature_columns):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        \n",
    "        if feature in baseline_data.columns and feature in current_data.columns:\n",
    "            # Plot histograms\n",
    "            axes[row, col].hist(baseline_data[feature], bins=30, alpha=0.7, label='Baseline', density=True)\n",
    "            axes[row, col].hist(current_data[feature], bins=30, alpha=0.7, label='Current', density=True)\n",
    "            \n",
    "            # Add statistics\n",
    "            baseline_mean = baseline_data[feature].mean()\n",
    "            current_mean = current_data[feature].mean()\n",
    "            \n",
    "            axes[row, col].axvline(baseline_mean, color='blue', linestyle='--', alpha=0.8, label=f'Baseline μ={baseline_mean:.2f}')\n",
    "            axes[row, col].axvline(current_mean, color='orange', linestyle='--', alpha=0.8, label=f'Current μ={current_mean:.2f}')\n",
    "            \n",
    "            axes[row, col].set_title(f'{feature}')\n",
    "            axes[row, col].set_xlabel(feature)\n",
    "            axes[row, col].set_ylabel('Density')\n",
    "            axes[row, col].legend()\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Distribution comparison plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot feature distributions\n",
    "feature_columns = [col for col in baseline_data.columns if col != 'generation']\n",
    "plot_feature_distributions(baseline_data, current_data, feature_columns[:6])  # Show first 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drift summary visualization\n",
    "def plot_drift_summary(drift_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Create summary visualization of drift detection results\n",
    "    \"\"\"\n",
    "    features = list(drift_results.keys())\n",
    "    \n",
    "    # Extract drift indicators\n",
    "    ks_drift = [drift_results[f]['kolmogorov_smirnov']['drift_detected'] for f in features]\n",
    "    chi2_drift = [drift_results[f]['chi_square']['drift_detected'] for f in features]\n",
    "    psi_values = [drift_results[f]['population_stability_index']['psi_value'] for f in features]\n",
    "    overall_drift = [drift_results[f]['overall_drift'] for f in features]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Drift Detection Summary', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Drift detection by test type\n",
    "    test_results = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'KS Test': ks_drift,\n",
    "        'Chi² Test': chi2_drift,\n",
    "        'Overall': overall_drift\n",
    "    })\n",
    "    \n",
    "    # Heatmap of drift detection\n",
    "    drift_matrix = test_results.set_index('Feature')[['KS Test', 'Chi² Test', 'Overall']].astype(int)\n",
    "    sns.heatmap(drift_matrix.T, annot=True, cmap='RdYlBu_r', cbar_kws={'label': 'Drift Detected'}, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Drift Detection by Test Type')\n",
    "    \n",
    "    # PSI values\n",
    "    axes[0, 1].bar(features, psi_values, color=['red' if psi > 0.2 else 'orange' if psi > 0.1 else 'green' for psi in psi_values])\n",
    "    axes[0, 1].axhline(y=0.1, color='orange', linestyle='--', label='Moderate Threshold')\n",
    "    axes[0, 1].axhline(y=0.2, color='red', linestyle='--', label='Significant Threshold')\n",
    "    axes[0, 1].set_title('Population Stability Index (PSI)')\n",
    "    axes[0, 1].set_ylabel('PSI Value')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Statistical changes (mean)\n",
    "    mean_changes = []\n",
    "    for feature in features:\n",
    "        baseline_mean = drift_results[feature]['baseline_stats']['mean']\n",
    "        current_mean = drift_results[feature]['current_stats']['mean']\n",
    "        change_pct = ((current_mean - baseline_mean) / baseline_mean) * 100\n",
    "        mean_changes.append(change_pct)\n",
    "    \n",
    "    colors = ['red' if abs(change) > 10 else 'orange' if abs(change) > 5 else 'green' for change in mean_changes]\n",
    "    axes[1, 0].bar(features, mean_changes, color=colors)\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[1, 0].set_title('Mean Change (%)')\n",
    "    axes[1, 0].set_ylabel('Percentage Change')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Drift summary pie chart\n",
    "    drift_counts = sum(overall_drift)\n",
    "    no_drift_counts = len(features) - drift_counts\n",
    "    \n",
    "    axes[1, 1].pie([drift_counts, no_drift_counts], \n",
    "                   labels=[f'Drift Detected\\n({drift_counts})', f'No Drift\\n({no_drift_counts})'],\n",
    "                   colors=['red', 'green'], autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 1].set_title('Overall Drift Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Drift summary plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create drift summary plot\n",
    "plot_drift_summary(drift_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate model predictions for performance monitoring\n",
    "def simulate_model_predictions(data, baseline_performance, add_degradation=False):\n",
    "    \"\"\"\n",
    "    Simulate model predictions with optional performance degradation\n",
    "    \"\"\"\n",
    "    # Use actual generation values as base\n",
    "    y_true = data['generation'].values\n",
    "    \n",
    "    # Simulate predictions based on baseline performance\n",
    "    baseline_rmse = baseline_performance['rmse']\n",
    "    \n",
    "    # Add noise to simulate prediction errors\n",
    "    noise_std = baseline_rmse\n",
    "    if add_degradation:\n",
    "        noise_std *= 1.5  # Increase error for degradation simulation\n",
    "    \n",
    "    noise = np.random.normal(0, noise_std, len(y_true))\n",
    "    y_pred = y_true + noise\n",
    "    \n",
    "    # Ensure non-negative predictions\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Initialize performance monitor\n",
    "performance_monitor = ModelPerformanceMonitor(baseline_metrics, threshold=PERFORMANCE_THRESHOLD)\n",
    "\n",
    "# Simulate multiple time periods\n",
    "print(\"📊 Simulating model performance over time...\")\n",
    "\n",
    "time_periods = [\n",
    "    ('Week 1', False),\n",
    "    ('Week 2', False),\n",
    "    ('Week 3', True),   # Introduce degradation\n",
    "    ('Week 4', True),\n",
    "]\n",
    "\n",
    "for period, degraded in time_periods:\n",
    "    # Simulate predictions\n",
    "    y_true, y_pred = simulate_model_predictions(current_data, baseline_metrics, add_degradation=degraded)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    current_metrics = performance_monitor.calculate_metrics(y_true, y_pred)\n",
    "    current_metrics['period'] = period\n",
    "    \n",
    "    # Add to history\n",
    "    performance_monitor.add_performance_record(current_metrics)\n",
    "    \n",
    "    # Check for degradation\n",
    "    degradation_flags = performance_monitor.detect_performance_degradation(current_metrics)\n",
    "    \n",
    "    print(f\"\\n{period}:\")\n",
    "    print(f\"  RMSE: {current_metrics['rmse']:.4f} (baseline: {baseline_metrics['rmse']:.4f})\")\n",
    "    print(f\"  R²: {current_metrics['r2_score']:.4f} (baseline: {baseline_metrics['r2_score']:.4f})\")\n",
    "    print(f\"  Degradation: {'YES' if degradation_flags['overall_degraded'] else 'NO'}\")\n",
    "    \n",
    "    if degradation_flags['overall_degraded']:\n",
    "        print(f\"    RMSE change: {degradation_flags['rmse_change_pct']:+.1f}%\")\n",
    "        print(f\"    R² change: {degradation_flags['r2_change_pct']:+.1f}%\")\n",
    "\n",
    "print(\"\\n✅ Performance monitoring simulation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance trends\n",
    "performance_monitor.plot_performance_trends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Automated Alerting System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertingSystem:\n",
    "    \"\"\"\n",
    "    Automated alerting system for model monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sns_topic_arn=None):\n",
    "        self.sns_topic_arn = sns_topic_arn\n",
    "        self.sns_client = boto3.client('sns') if sns_topic_arn else None\n",
    "        self.alert_history = []\n",
    "    \n",
    "    def create_drift_alert(self, drift_results, threshold=0.3):\n",
    "        \"\"\"\n",
    "        Create alert for data drift detection\n",
    "        \"\"\"\n",
    "        drifted_features = [feature for feature, result in drift_results.items() if result['overall_drift']]\n",
    "        drift_percentage = len(drifted_features) / len(drift_results) * 100\n",
    "        \n",
    "        if drift_percentage > threshold * 100:  # Convert threshold to percentage\n",
    "            alert = {\n",
    "                'type': 'DATA_DRIFT',\n",
    "                'severity': 'HIGH' if drift_percentage > 50 else 'MEDIUM',\n",
    "                'timestamp': datetime.now(),\n",
    "                'message': f'Data drift detected in {len(drifted_features)} features ({drift_percentage:.1f}%)',\n",
    "                'details': {\n",
    "                    'drifted_features': drifted_features,\n",
    "                    'drift_percentage': drift_percentage,\n",
    "                    'total_features': len(drift_results)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.send_alert(alert)\n",
    "            return alert\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_performance_alert(self, degradation_flags, current_metrics):\n",
    "        \"\"\"\n",
    "        Create alert for performance degradation\n",
    "        \"\"\"\n",
    "        if degradation_flags['overall_degraded']:\n",
    "            # Determine severity\n",
    "            max_degradation = max(\n",
    "                abs(degradation_flags['rmse_change_pct']),\n",
    "                abs(degradation_flags['r2_change_pct']),\n",
    "                abs(degradation_flags['mae_change_pct'])\n",
    "            )\n",
    "            \n",
    "            severity = 'CRITICAL' if max_degradation > 25 else 'HIGH' if max_degradation > 15 else 'MEDIUM'\n",
    "            \n",
    "            alert = {\n",
    "                'type': 'PERFORMANCE_DEGRADATION',\n",
    "                'severity': severity,\n",
    "                'timestamp': datetime.now(),\n",
    "                'message': f'Model performance degradation detected (max change: {max_degradation:.1f}%)',\n",
    "                'details': {\n",
    "                    'current_rmse': current_metrics['rmse'],\n",
    "                    'current_r2': current_metrics['r2_score'],\n",
    "                    'rmse_change_pct': degradation_flags['rmse_change_pct'],\n",
    "                    'r2_change_pct': degradation_flags['r2_change_pct'],\n",
    "                    'mae_change_pct': degradation_flags['mae_change_pct']\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.send_alert(alert)\n",
    "            return alert\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def send_alert(self, alert):\n",
    "        \"\"\"\n",
    "        Send alert via SNS or log locally\n",
    "        \"\"\"\n",
    "        self.alert_history.append(alert)\n",
    "        \n",
    "        # Format alert message\n",
    "        message = f\"\"\"\n",
    "🚨 SOLAR POWER ML MODEL ALERT\n",
    "\n",
    "Type: {alert['type']}\n",
    "Severity: {alert['severity']}\n",
    "Time: {alert['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Message: {alert['message']}\n",
    "\n",
    "Details:\n",
    "{json.dumps(alert['details'], indent=2)}\n",
    "\n",
    "Please investigate and take appropriate action.\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        if self.sns_client and self.sns_topic_arn:\n",
    "            try:\n",
    "                self.sns_client.publish(\n",
    "                    TopicArn=self.sns_topic_arn,\n",
    "                    Subject=f'Solar ML Alert: {alert[\"type\"]} - {alert[\"severity\"]}',\n",
    "                    Message=message\n",
    "                )\n",
    "                print(f\"✅ Alert sent via SNS: {alert['type']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to send SNS alert: {e}\")\n",
    "        else:\n",
    "            print(f\"🚨 ALERT: {alert['type']} - {alert['severity']}\")\n",
    "            print(message)\n",
    "    \n",
    "    def get_alert_summary(self):\n",
    "        \"\"\"\n",
    "        Get summary of all alerts\n",
    "        \"\"\"\n",
    "        if not self.alert_history:\n",
    "            return \"No alerts generated\"\n",
    "        \n",
    "        summary = []\n",
    "        summary.append(\"=\" * 50)\n",
    "        summary.append(\"ALERT SUMMARY\")\n",
    "        summary.append(\"=\" * 50)\n",
    "        summary.append(f\"Total Alerts: {len(self.alert_history)}\")\n",
    "        summary.append(\"\")\n",
    "        \n",
    "        for i, alert in enumerate(self.alert_history, 1):\n",
    "            summary.append(f\"{i}. {alert['type']} - {alert['severity']}\")\n",
    "            summary.append(f\"   Time: {alert['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            summary.append(f\"   Message: {alert['message']}\")\n",
    "            summary.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "# Initialize alerting system\n",
    "alerting_system = AlertingSystem()\n",
    "\n",
    "print(\"✅ AlertingSystem initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate alerts based on monitoring results\n",
    "print(\"🚨 Generating alerts based on monitoring results...\")\n",
    "\n",
    "# Check for drift alerts\n",
    "drift_alert = alerting_system.create_drift_alert(drift_results, threshold=0.2)\n",
    "\n",
    "# Check for performance alerts (using last performance record)\n",
    "if performance_monitor.performance_history:\n",
    "    latest_metrics = performance_monitor.performance_history[-1]\n",
    "    degradation_flags = performance_monitor.detect_performance_degradation(latest_metrics)\n",
    "    performance_alert = alerting_system.create_performance_alert(degradation_flags, latest_metrics)\n",
    "\n",
    "# Display alert summary\n",
    "print(\"\\n\" + alerting_system.get_alert_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Comprehensive Monitoring Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monitoring_report(drift_results, performance_history, alert_history, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate comprehensive monitoring report\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"SOLAR POWER ML MODEL MONITORING REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Model: {model_name}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"EXECUTIVE SUMMARY\")\n",
    "    report.append(\"-\" * 40)\n",
    "    \n",
    "    # Drift summary\n",
    "    total_features = len(drift_results)\n",
    "    drifted_features = sum(1 for result in drift_results.values() if result['overall_drift'])\n",
    "    drift_percentage = (drifted_features / total_features) * 100\n",
    "    \n",
    "    report.append(f\"Data Drift Status: {'⚠️ DETECTED' if drift_percentage > 20 else '✅ STABLE'}\")\n",
    "    report.append(f\"Features with Drift: {drifted_features}/{total_features} ({drift_percentage:.1f}%)\")\n",
    "    \n",
    "    # Performance summary\n",
    "    if performance_history:\n",
    "        latest_performance = performance_history[-1]\n",
    "        baseline_rmse = baseline_metrics['rmse']\n",
    "        current_rmse = latest_performance['rmse']\n",
    "        rmse_change = ((current_rmse - baseline_rmse) / baseline_rmse) * 100\n",
    "        \n",
    "        performance_status = '⚠️ DEGRADED' if abs(rmse_change) > 10 else '✅ STABLE'\n",
    "        report.append(f\"Performance Status: {performance_status}\")\n",
    "        report.append(f\"Current RMSE: {current_rmse:.4f} (baseline: {baseline_rmse:.4f}, change: {rmse_change:+.1f}%)\")\n",
    "    \n",
    "    # Alert summary\n",
    "    alert_count = len(alert_history)\n",
    "    critical_alerts = sum(1 for alert in alert_history if alert['severity'] == 'CRITICAL')\n",
    "    high_alerts = sum(1 for alert in alert_history if alert['severity'] == 'HIGH')\n",
    "    \n",
    "    report.append(f\"Active Alerts: {alert_count} (Critical: {critical_alerts}, High: {high_alerts})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Detailed Drift Analysis\n",
    "    report.append(\"DETAILED DRIFT ANALYSIS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    \n",
    "    for feature, results in drift_results.items():\n",
    "        if results['overall_drift']:\n",
    "            report.append(f\"\\n🔴 {feature} - DRIFT DETECTED\")\n",
    "            \n",
    "            # Test results\n",
    "            ks = results['kolmogorov_smirnov']\n",
    "            psi = results['population_stability_index']\n",
    "            \n",
    "            report.append(f\"  KS Test p-value: {ks['p_value']:.4f}\")\n",
    "            report.append(f\"  PSI Value: {psi['psi_value']:.4f} ({psi['interpretation']})\")\n",
    "            \n",
    "            # Statistical changes\n",
    "            baseline_stats = results['baseline_stats']\n",
    "            current_stats = results['current_stats']\n",
    "            mean_change = ((current_stats['mean'] - baseline_stats['mean']) / baseline_stats['mean']) * 100\n",
    "            \n",
    "            report.append(f\"  Mean change: {mean_change:+.2f}%\")\n",
    "            report.append(f\"  Baseline mean: {baseline_stats['mean']:.3f}\")\n",
    "            report.append(f\"  Current mean: {current_stats['mean']:.3f}\")\n",
    "    \n",
    "    # Performance Trend Analysis\n",
    "    if performance_history:\n",
    "        report.append(\"\\n\\nPERFORMANCE TREND ANALYSIS\")\n",
    "        report.append(\"-\" * 40)\n",
    "        \n",
    "        # Calculate trends\n",
    "        rmse_values = [p['rmse'] for p in performance_history]\n",
    "        r2_values = [p['r2_score'] for p in performance_history]\n",
    "        \n",
    "        rmse_trend = 'INCREASING' if rmse_values[-1] > rmse_values[0] else 'DECREASING'\n",
    "        r2_trend = 'INCREASING' if r2_values[-1] > r2_values[0] else 'DECREASING'\n",
    "        \n",
    "        report.append(f\"RMSE Trend: {rmse_trend}\")\n",
    "        report.append(f\"R² Trend: {r2_trend}\")\n",
    "        report.append(f\"Performance Records: {len(performance_history)}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"\\n\\nRECOMMENDATIONS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    \n",
    "    if drift_percentage > 30:\n",
    "        report.append(\"🔴 IMMEDIATE ACTION REQUIRED:\")\n",
    "        report.append(\"  - Investigate data sources for quality issues\")\n",
    "        report.append(\"  - Consider retraining the model with recent data\")\n",
    "        report.append(\"  - Review data collection and preprocessing pipelines\")\n",
    "    elif drift_percentage > 10:\n",
    "        report.append(\"🟡 MONITORING RECOMMENDED:\")\n",
    "        report.append(\"  - Increase monitoring frequency\")\n",
    "        report.append(\"  - Prepare for potential model retraining\")\n",
    "        report.append(\"  - Analyze root causes of drift\")\n",
    "    else:\n",
    "        report.append(\"🟢 CONTINUE MONITORING:\")\n",
    "        report.append(\"  - Maintain current monitoring schedule\")\n",
    "        report.append(\"  - Continue data quality checks\")\n",
    "    \n",
    "    if performance_history and abs(rmse_change) > 15:\n",
    "        report.append(\"\\n🔴 PERFORMANCE ACTION REQUIRED:\")\n",
    "        report.append(\"  - Immediate model retraining recommended\")\n",
    "        report.append(\"  - Review model deployment and inference pipeline\")\n",
    "        report.append(\"  - Consider A/B testing with new model\")\n",
    "    \n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"📄 Monitoring report saved to {save_path}\")\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "# Generate comprehensive report\n",
    "monitoring_report = generate_monitoring_report(\n",
    "    drift_results, \n",
    "    performance_monitor.performance_history, \n",
    "    alerting_system.alert_history,\n",
    "    save_path='../model_monitoring/reports/monitoring_report.txt'\n",
    ")\n",
    "\n",
    "print(monitoring_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Monitoring Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save monitoring results for future reference\n",
    "import os\n",
    "\n",
    "# Create monitoring results directory\n",
    "results_dir = '../model_monitoring/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save drift results\n",
    "drift_results_file = os.path.join(results_dir, f'drift_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "with open(drift_results_file, 'w') as f:\n",
    "    # Convert datetime objects to strings for JSON serialization\n",
    "    serializable_results = {}\n",
    "    for feature, result in drift_results.items():\n",
    "        serializable_results[feature] = {\n",
    "            'kolmogorov_smirnov': result['kolmogorov_smirnov'],\n",
    "            'chi_square': result['chi_square'],\n",
    "            'population_stability_index': result['population_stability_index'],\n",
    "            'baseline_stats': result['baseline_stats'],\n",
    "            'current_stats': result['current_stats'],\n",
    "            'overall_drift': result['overall_drift']\n",
    "        }\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "# Save performance history\n",
    "performance_file = os.path.join(results_dir, f'performance_history_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "with open(performance_file, 'w') as f:\n",
    "    serializable_performance = []\n",
    "    for record in performance_monitor.performance_history:\n",
    "        serializable_record = record.copy()\n",
    "        serializable_record['timestamp'] = record['timestamp'].isoformat()\n",
    "        serializable_performance.append(serializable_record)\n",
    "    json.dump(serializable_performance, f, indent=2)\n",
    "\n",
    "# Save alert history\n",
    "alerts_file = os.path.join(results_dir, f'alerts_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "with open(alerts_file, 'w') as f:\n",
    "    serializable_alerts = []\n",
    "    for alert in alerting_system.alert_history:\n",
    "        serializable_alert = alert.copy()\n",
    "        serializable_alert['timestamp'] = alert['timestamp'].isoformat()\n",
    "        serializable_alerts.append(serializable_alert)\n",
    "    json.dump(serializable_alerts, f, indent=2)\n",
    "\n",
    "print(\"✅ Monitoring results saved:\")\n",
    "print(f\"  - Drift results: {drift_results_file}\")\n",
    "print(f\"  - Performance history: {performance_file}\")\n",
    "print(f\"  - Alert history: {alerts_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated a comprehensive model monitoring and drift detection system for the solar power generation ML pipeline:\n",
    "\n",
    "### 🎯 **Key Components Implemented:**\n",
    "\n",
    "1. **Data Drift Detection**:\n",
    "   - Kolmogorov-Smirnov test for distribution comparison\n",
    "   - Chi-square test for categorical features\n",
    "   - Population Stability Index (PSI) calculation\n",
    "   - Statistical significance testing\n",
    "\n",
    "2. **Model Performance Monitoring**:\n",
    "   - Real-time performance metric calculation\n",
    "   - Performance degradation detection\n",
    "   - Trend analysis over time\n",
    "   - Baseline comparison\n",
    "\n",
    "3. **Automated Alerting System**:\n",
    "   - Configurable alert thresholds\n",
    "   - Multiple severity levels\n",
    "   - SNS integration for notifications\n",
    "   - Alert history tracking\n",
    "\n",
    "4. **Comprehensive Reporting**:\n",
    "   - Executive summary dashboards\n",
    "   - Detailed drift analysis\n",
    "   - Performance trend reports\n",
    "   - Actionable recommendations\n",
    "\n",
    "### 📊 **Monitoring Capabilities:**\n",
    "\n",
    "- **Real-time Drift Detection**: Continuous monitoring of feature distributions\n",
    "- **Performance Tracking**: RMSE, R², MAE, and MAPE monitoring\n",
    "- **Statistical Analysis**: Multiple statistical tests for robust detection\n",
    "- **Visualization**: Interactive plots and dashboards\n",
    "- **Historical Analysis**: Trend analysis and pattern recognition\n",
    "\n",
    "### 🚨 **Alert System Features:**\n",
    "\n",
    "- **Multi-level Alerts**: Critical, High, Medium severity levels\n",
    "- **Configurable Thresholds**: Customizable drift and performance thresholds\n",
    "- **Multiple Channels**: SNS, email, and local logging\n",
    "- **Rich Context**: Detailed alert information with actionable insights\n",
    "\n",
    "### 🔄 **Production Integration:**\n",
    "\n",
    "- **Automated Execution**: Can be scheduled to run automatically\n",
    "- **Cloud Integration**: AWS SNS for notifications\n",
    "- **Scalable Architecture**: Handles large datasets efficiently\n",
    "- **Persistent Storage**: Results saved for historical analysis\n",
    "\n",
    "### 📈 **Business Value:**\n",
    "\n",
    "- **Proactive Monitoring**: Early detection of model degradation\n",
    "- **Reduced Downtime**: Automated alerts prevent service disruption\n",
    "- **Data Quality Assurance**: Continuous data quality monitoring\n",
    "- **Compliance**: Audit trail for model governance\n",
    "- **Cost Optimization**: Prevents poor predictions and associated costs\n",
    "\n",
    "The monitoring system is now ready for production deployment and will continuously monitor the solar power generation model to ensure optimal performance and data quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.5"\n,
  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}

