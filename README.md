# Solar Power Generation ML Pipeline

A complete end-to-end machine learning pipeline for predicting solar farm power generation, built with Python and compatible with Amazon SageMaker.

## ðŸŒŸ Overview

This repository contains a comprehensive machine learning solution for predicting solar power generation using weather data and solar panel characteristics. The pipeline includes data preprocessing, model training, evaluation, deployment, automated retraining, and comprehensive monitoring capabilities.

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚â”€â”€â”€â–¶â”‚  Preprocessing  â”‚â”€â”€â”€â–¶â”‚ Model Training  â”‚
â”‚                 â”‚    â”‚   & Features    â”‚    â”‚   & Evaluation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Monitoring    â”‚â—€â”€â”€â”€â”‚   SageMaker     â”‚â—€â”€â”€â”€â”‚   Deployment    â”‚
â”‚  & Versioning   â”‚    â”‚   Deployment    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Automated     â”‚
                       â”‚   Retraining    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“Š Key Features

### Data Processing
- **Comprehensive Data Integration**: Combines weather data (temperature, irradiance, humidity, wind) with solar panel generation data
- **Feature Engineering**: Time-based features, lag variables, rolling statistics, and cyclical encoding
- **Data Validation**: Automated data quality checks and missing value handling
- **Scalable Processing**: Efficient processing of large datasets with memory optimization

### Model Development
- **Multiple Algorithms**: XGBoost, Random Forest, and Neural Network implementations
- **Hyperparameter Optimization**: Automated hyperparameter tuning with cross-validation
- **Performance Evaluation**: Comprehensive metrics including RMSE, MAE, RÂ², and residual analysis
- **Model Comparison**: Systematic comparison of different model architectures

### SageMaker Integration
- **Training Jobs**: Scalable training on AWS infrastructure
- **Model Registry**: Centralized model versioning and management
- **Endpoint Deployment**: Real-time inference endpoints with auto-scaling
- **Batch Transform**: Large-scale batch prediction capabilities

### Automated Retraining
- **Data Monitoring**: Automatic detection of new data in S3
- **Pipeline Orchestration**: Step Functions workflow for end-to-end automation
- **Performance Validation**: Automated model validation before deployment
- **Blue/Green Deployment**: Safe model updates with rollback capabilities

### Monitoring & Governance
- **Drift Detection**: Statistical tests for data and model drift
- **Performance Tracking**: Real-time monitoring of model performance
- **Audit Logging**: Comprehensive logging for compliance and debugging
- **Alert System**: Automated notifications for anomalies and failures

## ðŸš€ Quick Start

### Prerequisites
- Python 3.8+
- AWS CLI configured
- SageMaker execution role
- S3 bucket for data storage

### Installation
```bash
# Clone the repository
git clone https://github.com/your-username/solar-power-ml-pipeline.git
cd solar-power-ml-pipeline

# Install dependencies
pip install -r requirements.txt

# Set up AWS credentials
aws configure
```

### Basic Usage
```bash
# 1. Data preprocessing
python src/data/preprocessing_utils.py

# 2. Model training
python src/models/train_models.py

# 3. Model evaluation
jupyter notebook notebooks/04_model_evaluation.ipynb

# 4. Deploy to SageMaker
cd sagemaker_deployment
python scripts/deploy.py
```

## ðŸ“ Repository Structure

```
solar-power-ml-pipeline/
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for exploration and analysis
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_model_development.ipynb
â”‚   â””â”€â”€ 04_model_evaluation.ipynb
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data/                      # Data processing utilities
â”‚   â”œâ”€â”€ models/                    # Model training and evaluation
â”‚   â”œâ”€â”€ evaluation/                # Model evaluation utilities
â”‚   â””â”€â”€ utils/                     # General utilities
â”œâ”€â”€ sagemaker_deployment/          # SageMaker deployment files
â”‚   â”œâ”€â”€ code/                      # Training and inference scripts
â”‚   â”œâ”€â”€ config/                    # Deployment configurations
â”‚   â””â”€â”€ scripts/                   # Deployment automation
â”œâ”€â”€ automated_retraining/          # Automated retraining system
â”‚   â”œâ”€â”€ pipelines/                 # SageMaker Pipelines
â”‚   â”œâ”€â”€ step_functions/            # Step Functions workflows
â”‚   â”œâ”€â”€ lambda_functions/          # Lambda functions
â”‚   â””â”€â”€ monitoring/                # CloudWatch monitoring
â”œâ”€â”€ model_monitoring/              # Model monitoring and versioning
â”‚   â”œâ”€â”€ versioning/                # Model registry
â”‚   â”œâ”€â”€ drift_detection/           # Drift detection
â”‚   â”œâ”€â”€ performance_tracking/      # Performance monitoring
â”‚   â””â”€â”€ logging/                   # Logging system
â”œâ”€â”€ data/                          # Data storage
â”‚   â”œâ”€â”€ raw/                       # Raw data files
â”‚   â”œâ”€â”€ processed/                 # Processed datasets
â”‚   â””â”€â”€ baseline/                  # Baseline data for monitoring
â”œâ”€â”€ models/                        # Trained models
â”‚   â”œâ”€â”€ trained/                   # Model artifacts
â”‚   â””â”€â”€ artifacts/                 # Additional model files
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ unit/                      # Unit tests
â”‚   â”œâ”€â”€ integration/               # Integration tests
â”‚   â””â”€â”€ data/                      # Data validation tests
â”œâ”€â”€ docs/                          # Documentation
â”œâ”€â”€ scripts/                       # Utility scripts
â”œâ”€â”€ config/                        # Configuration files
â””â”€â”€ .github/                       # GitHub workflows
    â””â”€â”€ workflows/                 # CI/CD pipelines
```

## ðŸ”§ Configuration

### Environment Variables
```bash
export AWS_DEFAULT_REGION=us-east-1
export SAGEMAKER_ROLE_ARN=arn:aws:iam::YOUR_ACCOUNT:role/SageMakerExecutionRole
export S3_BUCKET=your-solar-data-bucket
export MODEL_REGISTRY_BUCKET=your-model-registry-bucket
```

### Model Configuration
Key parameters can be configured in `config/model_config.json`:
```json
{
  "model_type": "neural_network",
  "hyperparameters": {
    "hidden_layers": [100, 50, 25],
    "alpha": 0.001,
    "max_iter": 300
  },
  "performance_thresholds": {
    "min_r2_score": 0.95,
    "max_rmse": 0.5
  }
}
```

## ðŸ“ˆ Performance

### Model Results
The best performing model (Neural Network) achieves:
- **RMSE**: 0.2935 kWh
- **MAE**: 0.1142 kWh  
- **RÂ² Score**: 0.9905
- **MAPE**: 8.2%

### Benchmark Comparison
| Model | RMSE | MAE | RÂ² | Training Time |
|-------|------|-----|----|--------------| 
| Neural Network | 0.2935 | 0.1142 | 0.9905 | 45s |
| XGBoost | 0.4123 | 0.1402 | 0.9742 | 12s |
| Random Forest | 0.4310 | 0.1345 | 0.9721 | 8s |

## ðŸ”„ Automated Workflows

### Retraining Pipeline
The automated retraining system:
1. **Monitors** S3 for new data daily
2. **Validates** data quality and schema
3. **Trains** new model with updated dataset
4. **Evaluates** performance against thresholds
5. **Deploys** model if performance improves
6. **Notifies** stakeholders of results

### CI/CD Pipeline
GitHub Actions workflow for:
- Code quality checks (linting, formatting)
- Unit and integration testing
- Model validation tests
- Automated deployment to staging
- Performance regression testing

## ðŸ“Š Monitoring

### Key Metrics
- **Model Performance**: RMSE, MAE, RÂ² tracking over time
- **Data Quality**: Missing values, outliers, distribution changes
- **Operational**: Endpoint latency, throughput, error rates
- **Business**: Prediction accuracy, cost savings, energy forecasting

### Alerting
Automated alerts for:
- Model performance degradation (>10% RMSE increase)
- Data drift detection (p-value < 0.05)
- Endpoint failures or high latency
- Training job failures

## ðŸ§ª Testing

### Test Coverage
- **Unit Tests**: Individual function testing
- **Integration Tests**: End-to-end pipeline testing
- **Data Tests**: Data quality and schema validation
- **Model Tests**: Model performance and behavior testing

### Running Tests
```bash
# Run all tests
pytest tests/

# Run specific test categories
pytest tests/unit/
pytest tests/integration/
pytest tests/data/
```

## ðŸ“š Documentation

### Available Documentation
- [Data Processing Guide](docs/data_processing.md)
- [Model Development Guide](docs/model_development.md)
- [Deployment Guide](docs/deployment.md)
- [Monitoring Guide](docs/monitoring.md)
- [API Reference](docs/api_reference.md)

### Jupyter Notebooks
Interactive notebooks for:
- Data exploration and visualization
- Feature engineering experiments
- Model development and tuning
- Performance analysis and interpretation

## ðŸ¤ Contributing

### Development Setup
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install
```

### Contribution Guidelines
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- Weather data provided by [Hong Kong Observatory](https://www.hko.gov.hk/)
- Solar panel data from university campus installations
- AWS SageMaker team for excellent documentation and examples
- Open source community for the amazing ML libraries

## ðŸ“ž Support

For questions, issues, or contributions:
- **Issues**: [GitHub Issues](https://github.com/your-username/solar-power-ml-pipeline/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/solar-power-ml-pipeline/discussions)
- **Email**: your-email@example.com

## ðŸ”— Related Projects

- [Solar Power Forecasting Dashboard](https://github.com/your-username/solar-dashboard)
- [Weather Data Pipeline](https://github.com/your-username/weather-pipeline)
- [Energy Management System](https://github.com/your-username/energy-management)

---

**Built with â¤ï¸ by [Your Name] | Powered by AWS SageMaker**
